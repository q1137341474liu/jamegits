{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e04cde9-1675-492b-83b7-2db92d31e5c3",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45da03fb-ee51-4ce6-a3f3-5a5692095cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500 — MAE: 5861.5300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     73\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 74\u001b[0m     total_mae \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m xb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     75\u001b[0m epoch_mae \u001b[38;5;241m=\u001b[39m total_mae \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df  = pd.read_csv('test.csv')\n",
    "\n",
    "# 2. Prepare features & target\n",
    "features = [c for c in train_df.columns if c not in ('id', 'Row#', 'yield')]\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df['yield'].values\n",
    "X_test  = test_df[features].values\n",
    "test_ids = test_df['id'].values\n",
    "\n",
    "# 3. Scale inputs\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# 4. To torch tensors: shape (N, seq_len=F, input_size=1)\n",
    "#    LSTM with batch_first=True expects (batch, seq_len, input_size)\n",
    "X_train_t = torch.from_numpy(X_train).float().unsqueeze(-1)\n",
    "y_train_t = torch.from_numpy(y_train).float().unsqueeze(1)\n",
    "X_test_t  = torch.from_numpy(X_test).float().unsqueeze(-1)\n",
    "\n",
    "# 5. DataLoader\n",
    "train_ds     = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# 6. Define an LSTM regressor\n",
    "class RNNRegressor(nn.Module):\n",
    "    def __init__(self, seq_len, hidden_size=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        # Map from hidden state to a single output\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, 1)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # h_n: (num_layers, batch, hidden_size)\n",
    "        last_h = h_n[-1]                    # (batch, hidden_size)\n",
    "        return self.fc(last_h)              # (batch, 1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = RNNRegressor(seq_len=len(features)).to(device)\n",
    "\n",
    "# 7. MAE loss & optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 8. Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    total_mae = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_mae += loss.item() * xb.size(0)\n",
    "    epoch_mae = total_mae / len(train_loader.dataset)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch:2d}/{num_epochs} — MAE: {epoch_mae:.4f}')\n",
    "\n",
    "# 9. Inference on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_t = X_test_t.to(device)\n",
    "    y_pred   = model(X_test_t).cpu().numpy().flatten()\n",
    "\n",
    "# 10. Save submission\n",
    "submission = pd.DataFrame({\n",
    "    'id':    test_ids,\n",
    "    'yield': y_pred\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Done → predictions saved to submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c37cd9-6540-42c1-8c52-b666ac1c15fd",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a79af45d-5270-4db2-a4b3-ba2f822c86b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/500 — Train MAE: 5983.29  |  Val MAE: 5998.16\n",
      "Epoch 10/500 — Train MAE: 5919.96  |  Val MAE: 5926.67\n",
      "Epoch 15/500 — Train MAE: 5825.46  |  Val MAE: 5827.65\n",
      "Epoch 20/500 — Train MAE: 5703.88  |  Val MAE: 5708.78\n",
      "Epoch 25/500 — Train MAE: 5553.51  |  Val MAE: 5554.03\n",
      "Epoch 30/500 — Train MAE: 5371.27  |  Val MAE: 5367.82\n",
      "Epoch 35/500 — Train MAE: 5165.43  |  Val MAE: 5137.62\n",
      "Epoch 40/500 — Train MAE: 4922.79  |  Val MAE: 4890.68\n",
      "Epoch 45/500 — Train MAE: 4646.46  |  Val MAE: 4618.26\n",
      "Epoch 50/500 — Train MAE: 4346.60  |  Val MAE: 4323.14\n",
      "Epoch 55/500 — Train MAE: 4023.11  |  Val MAE: 3984.50\n",
      "Epoch 60/500 — Train MAE: 3675.74  |  Val MAE: 3625.70\n",
      "Epoch 65/500 — Train MAE: 3302.23  |  Val MAE: 3229.27\n",
      "Epoch 70/500 — Train MAE: 2888.48  |  Val MAE: 2848.80\n",
      "Epoch 75/500 — Train MAE: 2469.20  |  Val MAE: 2311.55\n",
      "Epoch 80/500 — Train MAE: 2038.49  |  Val MAE: 2038.21\n",
      "Epoch 85/500 — Train MAE: 1651.91  |  Val MAE: 1579.85\n",
      "Epoch 90/500 — Train MAE: 1308.31  |  Val MAE: 1205.12\n",
      "Epoch 95/500 — Train MAE: 1074.99  |  Val MAE: 1038.81\n",
      "Epoch 100/500 — Train MAE: 802.26  |  Val MAE: 583.27\n",
      "Epoch 105/500 — Train MAE: 651.47  |  Val MAE: 404.43\n",
      "Epoch 110/500 — Train MAE: 604.30  |  Val MAE: 338.44\n",
      "Epoch 115/500 — Train MAE: 653.87  |  Val MAE: 380.82\n",
      "Epoch 120/500 — Train MAE: 633.79  |  Val MAE: 281.08\n",
      "Epoch 125/500 — Train MAE: 624.74  |  Val MAE: 355.73\n",
      "Epoch 130/500 — Train MAE: 601.60  |  Val MAE: 267.68\n",
      "Epoch 135/500 — Train MAE: 586.24  |  Val MAE: 267.63\n",
      "Epoch 140/500 — Train MAE: 588.52  |  Val MAE: 261.55\n",
      "Epoch 145/500 — Train MAE: 583.55  |  Val MAE: 273.48\n",
      "Epoch 150/500 — Train MAE: 582.67  |  Val MAE: 262.64\n",
      "Epoch 155/500 — Train MAE: 580.94  |  Val MAE: 277.04\n",
      "Epoch 160/500 — Train MAE: 587.93  |  Val MAE: 257.71\n",
      "Epoch 165/500 — Train MAE: 587.47  |  Val MAE: 257.49\n",
      "Epoch 170/500 — Train MAE: 571.50  |  Val MAE: 268.30\n",
      "Epoch 175/500 — Train MAE: 587.45  |  Val MAE: 257.47\n",
      "Epoch 180/500 — Train MAE: 576.26  |  Val MAE: 262.52\n",
      "Epoch 185/500 — Train MAE: 581.54  |  Val MAE: 258.59\n",
      "Epoch 190/500 — Train MAE: 593.85  |  Val MAE: 259.04\n",
      "Epoch 195/500 — Train MAE: 580.20  |  Val MAE: 260.57\n",
      "Epoch 200/500 — Train MAE: 588.88  |  Val MAE: 259.19\n",
      "Epoch 205/500 — Train MAE: 582.34  |  Val MAE: 258.66\n",
      "Epoch 210/500 — Train MAE: 586.17  |  Val MAE: 270.87\n",
      "Epoch 215/500 — Train MAE: 580.38  |  Val MAE: 258.52\n",
      "Epoch 220/500 — Train MAE: 580.20  |  Val MAE: 257.62\n",
      "Epoch 225/500 — Train MAE: 583.14  |  Val MAE: 257.21\n",
      "Epoch 230/500 — Train MAE: 588.91  |  Val MAE: 266.24\n",
      "Epoch 235/500 — Train MAE: 579.95  |  Val MAE: 259.17\n",
      "Epoch 240/500 — Train MAE: 581.92  |  Val MAE: 265.46\n",
      "Epoch 245/500 — Train MAE: 587.56  |  Val MAE: 260.57\n",
      "Epoch 250/500 — Train MAE: 576.17  |  Val MAE: 257.80\n",
      "Epoch 255/500 — Train MAE: 582.24  |  Val MAE: 261.10\n",
      "Epoch 260/500 — Train MAE: 587.13  |  Val MAE: 272.50\n",
      "Epoch 265/500 — Train MAE: 587.78  |  Val MAE: 267.09\n",
      "Epoch 270/500 — Train MAE: 579.98  |  Val MAE: 256.75\n",
      "Epoch 275/500 — Train MAE: 586.82  |  Val MAE: 259.31\n",
      "Epoch 280/500 — Train MAE: 578.04  |  Val MAE: 256.86\n",
      "Epoch 285/500 — Train MAE: 582.96  |  Val MAE: 264.75\n",
      "Epoch 290/500 — Train MAE: 587.90  |  Val MAE: 270.79\n",
      "Epoch 295/500 — Train MAE: 579.87  |  Val MAE: 266.52\n",
      "Epoch 300/500 — Train MAE: 590.18  |  Val MAE: 259.04\n",
      "Epoch 305/500 — Train MAE: 587.65  |  Val MAE: 275.26\n",
      "Epoch 310/500 — Train MAE: 576.29  |  Val MAE: 268.79\n",
      "Epoch 315/500 — Train MAE: 584.08  |  Val MAE: 268.99\n",
      "Epoch 320/500 — Train MAE: 572.39  |  Val MAE: 260.19\n",
      "Epoch 325/500 — Train MAE: 582.15  |  Val MAE: 261.32\n",
      "Epoch 330/500 — Train MAE: 569.58  |  Val MAE: 265.18\n",
      "Epoch 335/500 — Train MAE: 589.21  |  Val MAE: 264.72\n",
      "Epoch 340/500 — Train MAE: 578.59  |  Val MAE: 261.01\n",
      "Epoch 345/500 — Train MAE: 573.58  |  Val MAE: 260.04\n",
      "Epoch 350/500 — Train MAE: 577.25  |  Val MAE: 266.61\n",
      "Epoch 355/500 — Train MAE: 584.50  |  Val MAE: 256.53\n",
      "Epoch 360/500 — Train MAE: 579.23  |  Val MAE: 276.78\n",
      "Epoch 365/500 — Train MAE: 582.34  |  Val MAE: 269.93\n",
      "Epoch 370/500 — Train MAE: 583.58  |  Val MAE: 263.78\n",
      "Epoch 375/500 — Train MAE: 590.34  |  Val MAE: 264.78\n",
      "Epoch 380/500 — Train MAE: 586.40  |  Val MAE: 261.81\n",
      "Epoch 385/500 — Train MAE: 587.58  |  Val MAE: 256.07\n",
      "Epoch 390/500 — Train MAE: 579.20  |  Val MAE: 256.60\n",
      "Epoch 395/500 — Train MAE: 584.34  |  Val MAE: 257.15\n",
      "Epoch 400/500 — Train MAE: 582.68  |  Val MAE: 260.36\n",
      "Epoch 405/500 — Train MAE: 585.00  |  Val MAE: 264.43\n",
      "Epoch 410/500 — Train MAE: 578.54  |  Val MAE: 262.84\n",
      "Epoch 415/500 — Train MAE: 576.71  |  Val MAE: 257.85\n",
      "Epoch 420/500 — Train MAE: 581.73  |  Val MAE: 262.77\n",
      "Epoch 425/500 — Train MAE: 574.90  |  Val MAE: 257.84\n",
      "Epoch 430/500 — Train MAE: 590.10  |  Val MAE: 261.09\n",
      "Epoch 435/500 — Train MAE: 585.23  |  Val MAE: 261.45\n",
      "Epoch 440/500 — Train MAE: 575.34  |  Val MAE: 258.12\n",
      "Epoch 445/500 — Train MAE: 577.87  |  Val MAE: 258.45\n",
      "Epoch 450/500 — Train MAE: 582.30  |  Val MAE: 263.41\n",
      "Epoch 455/500 — Train MAE: 581.51  |  Val MAE: 257.11\n",
      "Epoch 460/500 — Train MAE: 586.58  |  Val MAE: 271.35\n",
      "Epoch 465/500 — Train MAE: 577.42  |  Val MAE: 282.52\n",
      "Epoch 470/500 — Train MAE: 579.77  |  Val MAE: 269.13\n",
      "Epoch 475/500 — Train MAE: 578.60  |  Val MAE: 257.90\n",
      "Epoch 480/500 — Train MAE: 592.63  |  Val MAE: 267.58\n",
      "Epoch 485/500 — Train MAE: 580.14  |  Val MAE: 259.15\n",
      "Epoch 490/500 — Train MAE: 576.05  |  Val MAE: 260.25\n",
      "Epoch 495/500 — Train MAE: 587.25  |  Val MAE: 266.55\n",
      "Epoch 500/500 — Train MAE: 586.28  |  Val MAE: 269.13\n",
      "▶ Best validation MAE: 256.07\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load & split data\n",
    "df = pd.read_csv('train.csv')\n",
    "features = [c for c in df.columns if c not in ('id','Row#','yield')]\n",
    "X = df[features].values\n",
    "y = df['yield'].values\n",
    "\n",
    "# simple 80/20 train/val split\n",
    "dataset = TensorDataset(\n",
    "    torch.from_numpy(StandardScaler().fit_transform(X)).float().unsqueeze(-1),\n",
    "    torch.from_numpy(y).float().unsqueeze(1)\n",
    ")\n",
    "n_val = int(len(dataset)*0.2)\n",
    "n_tr  = len(dataset) - n_val\n",
    "train_ds, val_ds = random_split(dataset, [n_tr, n_val])\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64)\n",
    "\n",
    "# 2. Define a beefed‑up LSTM regressor\n",
    "class RNNRegressor(nn.Module):\n",
    "    def __init__(self, seq_len, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers>1 else 0.0\n",
    "        )\n",
    "        # after LSTM, combine last‑layer forward & backward\n",
    "        self.fc1 = nn.Linear(hidden_size * self.num_directions, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, 1)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # h_n: (num_layers * num_directions, batch, hidden_size)\n",
    "        # take the last layer's hidden states:\n",
    "        last = h_n.view(self.lstm.num_layers, self.num_directions, x.size(0), \n",
    "                        self.lstm.hidden_size)[-1]\n",
    "        # last shape: (num_directions, batch, hidden_size)\n",
    "        last = last.transpose(0,1).contiguous().view(x.size(0), -1)\n",
    "        x = torch.relu(self.bn1(self.fc1(last)))\n",
    "        x = self.drop(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# 3. Instantiate\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNNRegressor(seq_len=len(features)).to(device)\n",
    "\n",
    "# 4. Loss, optimizer, scheduler\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       mode='min',\n",
    "                                                       factor=0.5,\n",
    "                                                       patience=5,\n",
    "                                                       verbose=True)\n",
    "\n",
    "# 5. Training + validation loops\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = criterion(preds, yb)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "num_epochs = 500\n",
    "best_val = float('inf')\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_mae = run_epoch(train_loader, train=True)\n",
    "    val_mae   = run_epoch(val_loader,   train=False)\n",
    "    scheduler.step(val_mae)\n",
    "\n",
    "    if val_mae < best_val:\n",
    "        best_val = val_mae\n",
    "        torch.save(model.state_dict(), 'best_rnn.pth')\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:2d}/{num_epochs} — \"\n",
    "              f\"Train MAE: {train_mae:.2f}  |  Val MAE: {val_mae:.2f}\")\n",
    "\n",
    "print(f\"▶ Best validation MAE: {best_val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb164f4e-fe98-4a79-b986-1ad065496c1e",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb87c4e-2879-4bd6-9a04-927bb918e8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/60 — Train MAE: 5985.89 | Val MAE: 6003.07\n",
      "Epoch 10/60 — Train MAE: 5929.43 | Val MAE: 5949.12\n",
      "Epoch 15/60 — Train MAE: 5850.60 | Val MAE: 5859.79\n",
      "Epoch 20/60 — Train MAE: 5742.09 | Val MAE: 5753.28\n",
      "Epoch 25/60 — Train MAE: 5610.13 | Val MAE: 5622.58\n",
      "Epoch 30/60 — Train MAE: 5447.29 | Val MAE: 5450.65\n",
      "Epoch 35/60 — Train MAE: 5277.47 | Val MAE: 5296.85\n",
      "Epoch 40/60 — Train MAE: 5056.13 | Val MAE: 5056.69\n",
      "Epoch 45/60 — Train MAE: 4835.32 | Val MAE: 4838.67\n",
      "Epoch 50/60 — Train MAE: 4575.81 | Val MAE: 4590.76\n",
      "Epoch 55/60 — Train MAE: 4325.42 | Val MAE: 4347.83\n",
      "Epoch 60/60 — Train MAE: 3950.30 | Val MAE: 3908.50\n",
      "Best Validation MAE: 3908.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load & preprocess data\n",
    "df = pd.read_csv('train.csv')\n",
    "features = [c for c in df.columns if c not in ('id', 'Row#', 'yield')]\n",
    "X = df[features].values\n",
    "y = df['yield'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Prepare PyTorch datasets\n",
    "data = TensorDataset(\n",
    "    torch.from_numpy(X_scaled).float().unsqueeze(-1),\n",
    "    torch.from_numpy(y).float().unsqueeze(1)\n",
    ")\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size = len(data) - train_size\n",
    "train_ds, val_ds = random_split(data, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "# Define LSTM regressor\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, seq_len, hidden_size=128, num_layers=2, dropout=0.2, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional, batch_first=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size * self.num_directions, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # Extract last layer's hidden state\n",
    "        h_last = h_n.view(self.lstm.num_layers, self.num_directions, x.size(0), self.lstm.hidden_size)[-1]\n",
    "        h_last = h_last.transpose(0, 1).reshape(x.size(0), -1)\n",
    "        x = torch.relu(self.bn1(self.fc1(h_last)))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMRegressor(seq_len=len(features)).to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Training & validation loop\n",
    "best_val_mae = float('inf')\n",
    "for epoch in range(1, 61):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_mae = train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "    val_mae = val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    scheduler.step(val_mae)\n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        torch.save(model.state_dict(), 'best_lstm.pth')\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch:2d}/60 — Train MAE: {train_mae:.2f} | Val MAE: {val_mae:.2f}')\n",
    "\n",
    "print(f'Best Validation MAE: {best_val_mae:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5203a9c2-1041-4a17-b7c5-64205fd8705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/60 — Train MAE: 5756.14 | Val MAE: 5691.66\n",
      "Epoch 10/60 — Train MAE: 4612.38 | Val MAE: 4429.66\n",
      "Epoch 15/60 — Train MAE: 1998.56 | Val MAE: 1679.00\n",
      "Epoch 20/60 — Train MAE: 551.95 | Val MAE: 317.83\n",
      "Epoch 25/60 — Train MAE: 508.10 | Val MAE: 273.05\n",
      "Epoch 30/60 — Train MAE: 493.30 | Val MAE: 288.25\n",
      "Epoch 35/60 — Train MAE: 492.51 | Val MAE: 259.87\n",
      "Epoch 40/60 — Train MAE: 498.96 | Val MAE: 276.74\n",
      "Epoch 45/60 — Train MAE: 495.81 | Val MAE: 261.67\n",
      "Epoch 50/60 — Train MAE: 492.73 | Val MAE: 264.46\n",
      "Epoch 55/60 — Train MAE: 491.26 | Val MAE: 259.08\n",
      "Epoch 60/60 — Train MAE: 493.00 | Val MAE: 264.31\n",
      "Epoch 65/60 — Train MAE: 485.79 | Val MAE: 257.98\n",
      "Epoch 70/60 — Train MAE: 484.93 | Val MAE: 260.12\n",
      "Epoch 75/60 — Train MAE: 491.37 | Val MAE: 258.15\n",
      "Epoch 80/60 — Train MAE: 488.20 | Val MAE: 259.00\n",
      "Epoch 85/60 — Train MAE: 485.15 | Val MAE: 258.47\n",
      "Epoch 90/60 — Train MAE: 490.70 | Val MAE: 258.88\n",
      "Epoch 95/60 — Train MAE: 485.94 | Val MAE: 259.55\n",
      "Epoch 100/60 — Train MAE: 485.24 | Val MAE: 259.43\n",
      "▶ Best Validation MAE: 255.52\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load & preprocess data\n",
    "df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "features = [c for c in df.columns if c not in ('id', 'Row#', 'yield')]\n",
    "X = df[features].values\n",
    "y = df['yield'].values\n",
    "X_test = test_df[features].values\n",
    "test_ids = test_df['id'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2. Prepare PyTorch datasets\n",
    "data = TensorDataset(\n",
    "    torch.from_numpy(X).float().unsqueeze(-1),\n",
    "    torch.from_numpy(y).float().unsqueeze(1)\n",
    ")\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size   = len(data) - train_size\n",
    "train_ds, val_ds = random_split(data, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64)\n",
    "test_loader  = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_test).float().unsqueeze(-1)),\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# 3. Positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "\n",
    "# 4. Transformer regressor\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, seq_len, d_model=64, nhead=8,\n",
    "                 num_layers=2, dim_ff=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(1, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=seq_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer,\n",
    "                                             num_layers=num_layers)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, 1)\n",
    "        x = self.embed(x)                        # (batch, seq_len, d_model)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.encoder(x)                      # (batch, seq_len, d_model)\n",
    "        x = x.mean(dim=1)                        # global average pool\n",
    "        return self.regressor(x)                 # (batch, 1)\n",
    "\n",
    "# 5. Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerRegressor(seq_len=len(features)).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# 6. Training & validation loops\n",
    "best_val_mae = float('inf')\n",
    "for epoch in range(1, 101):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_mae = train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            val_loss += criterion(preds, yb).item() * xb.size(0)\n",
    "    val_mae = val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    scheduler.step(val_mae)\n",
    "    \n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        torch.save(model.state_dict(), 'best_transformer.pth')\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch:2d}/60 — Train MAE: {train_mae:.2f} | Val MAE: {val_mae:.2f}')\n",
    "\n",
    "print(f\"▶ Best Validation MAE: {best_val_mae:.2f}\")\n",
    "\n",
    "# 7. Inference on test set\n",
    "model.load_state_dict(torch.load('best_transformer.pth'))\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for xb in test_loader:\n",
    "        xb = xb[0].to(device)\n",
    "        preds.append(model(xb).cpu().numpy())\n",
    "preds = np.vstack(preds).flatten()\n",
    "\n",
    "# 8. Save submission\n",
    "pd.DataFrame({'id': test_ids, 'yield': preds}) \\\n",
    "  .to_csv('submission.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb0998-b88d-4d28-91d2-9e9bef7b41bd",
   "metadata": {},
   "source": [
    "# Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "834a7874-4d44-450e-83ed-990ea03d4461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/201 - Train MAE: 5364.33 | Val MAE: 5258.50\n",
      "Epoch 10/201 - Train MAE: 2463.26 | Val MAE: 2088.70\n",
      "Epoch 15/201 - Train MAE: 674.75 | Val MAE: 404.18\n",
      "Epoch 20/201 - Train MAE: 603.79 | Val MAE: 292.61\n",
      "Epoch 25/201 - Train MAE: 594.34 | Val MAE: 308.82\n",
      "Epoch 30/201 - Train MAE: 587.33 | Val MAE: 303.68\n",
      "Epoch 35/201 - Train MAE: 598.77 | Val MAE: 282.88\n",
      "Epoch 40/201 - Train MAE: 578.86 | Val MAE: 270.41\n",
      "Epoch 45/201 - Train MAE: 585.12 | Val MAE: 280.59\n",
      "Epoch 50/201 - Train MAE: 581.04 | Val MAE: 277.02\n",
      "Epoch 55/201 - Train MAE: 584.10 | Val MAE: 277.08\n",
      "Epoch 60/201 - Train MAE: 564.65 | Val MAE: 267.08\n",
      "Epoch 65/201 - Train MAE: 566.26 | Val MAE: 286.65\n",
      "Epoch 70/201 - Train MAE: 561.76 | Val MAE: 272.93\n",
      "Epoch 75/201 - Train MAE: 569.68 | Val MAE: 268.95\n",
      "Epoch 80/201 - Train MAE: 558.34 | Val MAE: 269.91\n",
      "Epoch 85/201 - Train MAE: 556.47 | Val MAE: 270.50\n",
      "Epoch 90/201 - Train MAE: 562.83 | Val MAE: 264.56\n",
      "Epoch 95/201 - Train MAE: 566.45 | Val MAE: 264.71\n",
      "Best Validation MAE: 263.05\n",
      "Done → saved hybrid model predictions to submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load & preprocess data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df  = pd.read_csv('test.csv')\n",
    "features = [c for c in train_df.columns if c not in ('id','Row#','yield')]\n",
    "\n",
    "X = train_df[features].values\n",
    "y = train_df['yield'].values\n",
    "X_test = test_df[features].values\n",
    "test_ids = test_df['id'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X      = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2. Create PyTorch datasets and loaders\n",
    "data = TensorDataset(torch.from_numpy(X).float().unsqueeze(-1),\n",
    "                     torch.from_numpy(y).float().unsqueeze(1))\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size   = len(data) - train_size\n",
    "train_ds, val_ds = random_split(data, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64)\n",
    "test_loader  = DataLoader(TensorDataset(torch.from_numpy(X_test).float().unsqueeze(-1)),\n",
    "                          batch_size=64)\n",
    "\n",
    "# 3. Define a CNN + LSTM hybrid regressor\n",
    "class HybridRegressor(nn.Module):\n",
    "    def __init__(self, seq_len, cnn_channels=(16,32), \n",
    "                 lstm_hidden=64, lstm_layers=2, bidir=True):\n",
    "        super().__init__()\n",
    "        # CNN branch\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(1, cnn_channels[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(cnn_channels[0], cnn_channels[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        # LSTM branch (on CNN features)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_channels[1],\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidir,\n",
    "            dropout=0.2 if lstm_layers>1 else 0.0\n",
    "        )\n",
    "        self.bidir = bidir\n",
    "        # Final MLP\n",
    "        mlp_in = lstm_hidden * (2 if bidir else 1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(mlp_in, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, 1) -> CNN expects (batch, 1, seq_len)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.cnn(x)\n",
    "        # x: (batch, channels, seq_len/4) -> back to (batch, seq_len/4, channels)\n",
    "        x = x.transpose(1,2)\n",
    "        # LSTM\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # pick last layer hidden: shape (num_layers * directions, batch, hidden)\n",
    "        if self.bidir:\n",
    "            # get last forward & backward hidden\n",
    "            h_last = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h_last = h_n[-1]\n",
    "        # final regressor\n",
    "        return self.mlp(h_last)\n",
    "\n",
    "# 4. Setup training components\n",
    "device   = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model    = HybridRegressor(seq_len=len(features)).to(device)\n",
    "criterion= nn.L1Loss()\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=5.1e-4, weight_decay=0.5e-5)\n",
    "scheduler= torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                     factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# 5. Training & validation loop\n",
    "best_val = float('inf')\n",
    "for epoch in range(1, 100):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_train = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_train += loss.item() * xb.size(0)\n",
    "    train_mae = total_train / len(train_loader.dataset)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    total_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            total_val += criterion(preds, yb).item() * xb.size(0)\n",
    "    val_mae = total_val / len(val_loader.dataset)\n",
    "\n",
    "    scheduler.step(val_mae)\n",
    "    if val_mae < best_val:\n",
    "        best_val = val_mae\n",
    "        torch.save(model.state_dict(), 'best_hybrid.pth')\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:2d}/201 - Train MAE: {train_mae:.2f} | Val MAE: {val_mae:.2f}\")\n",
    "\n",
    "print(f\"Best Validation MAE: {best_val:.2f}\")\n",
    "\n",
    "# 6. Inference on test set\n",
    "model.load_state_dict(torch.load('best_hybrid.pth'))\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for xb in test_loader:\n",
    "        xb = xb[0].to(device)\n",
    "        preds.append(model(xb).cpu().numpy())\n",
    "preds = np.vstack(preds).flatten()\n",
    "\n",
    "# 7. Save submission\n",
    "pd.DataFrame({'id': test_ids, 'yield': preds}).to_csv('submission.csv', index=False)\n",
    "print(\"Done → saved hybrid model predictions to submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a575fcb-df9b-4b5a-8f57-109206aef5db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
