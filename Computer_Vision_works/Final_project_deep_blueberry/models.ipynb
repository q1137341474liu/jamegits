{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a610be25-c7c6-49ed-9588-5b2bf7b44fe5",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4a93dc-a7f5-4f03-aa18-821fb9e6291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04cde9-1675-492b-83b7-2db92d31e5c3",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45da03fb-ee51-4ce6-a3f3-5a5692095cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/200 - Train MAE: 513.9194 - Val MAE: 287.4022\n",
      "Epoch  20/200 - Train MAE: 490.1140 - Val MAE: 284.1623\n",
      "Epoch  30/200 - Train MAE: 485.4239 - Val MAE: 285.0510\n",
      "Epoch  40/200 - Train MAE: 477.6255 - Val MAE: 263.8176\n",
      "Epoch  50/200 - Train MAE: 481.2920 - Val MAE: 261.5328\n",
      "Epoch  60/200 - Train MAE: 473.4218 - Val MAE: 274.1691\n",
      "Epoch  70/200 - Train MAE: 477.3768 - Val MAE: 261.4722\n",
      "Epoch  80/200 - Train MAE: 479.1592 - Val MAE: 262.4379\n",
      "Epoch  90/200 - Train MAE: 471.6675 - Val MAE: 260.6419\n",
      "Epoch 100/200 - Train MAE: 473.7440 - Val MAE: 260.6624\n",
      "Epoch 110/200 - Train MAE: 470.4218 - Val MAE: 259.9990\n",
      "Epoch 120/200 - Train MAE: 472.3900 - Val MAE: 260.8253\n",
      "Epoch 130/200 - Train MAE: 476.7619 - Val MAE: 263.0355\n",
      "Epoch 140/200 - Train MAE: 474.2400 - Val MAE: 262.6856\n",
      "Epoch 150/200 - Train MAE: 485.3300 - Val MAE: 260.3950\n",
      "Epoch 160/200 - Train MAE: 478.0532 - Val MAE: 262.3427\n",
      "Epoch 170/200 - Train MAE: 480.0227 - Val MAE: 260.6604\n",
      "Epoch 180/200 - Train MAE: 468.0858 - Val MAE: 260.0551\n",
      "Epoch 190/200 - Train MAE: 471.9042 - Val MAE: 265.7434\n",
      "Epoch 200/200 - Train MAE: 473.7206 - Val MAE: 261.5668\n",
      "Done → CNN predictions saved.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. Load and prepare data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "features = [c for c in train_df.columns if c not in ('id', 'Row#', 'yield')]\n",
    "X = train_df[features].values\n",
    "y = train_df['yield'].values\n",
    "X_test = test_df[features].values\n",
    "test_ids = test_df['id'].values\n",
    "\n",
    "# 2. Scale data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 3. Train-val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Convert to PyTorch tensors - reshape for CNN (batch, channels, length)\n",
    "X_train_t = torch.from_numpy(X_train).float().unsqueeze(1)  # Add channel dimension\n",
    "y_train_t = torch.from_numpy(y_train).float().unsqueeze(1)\n",
    "X_val_t = torch.from_numpy(X_val).float().unsqueeze(1)\n",
    "y_val_t = torch.from_numpy(y_val).float().unsqueeze(1)\n",
    "X_test_t = torch.from_numpy(X_test).float().unsqueeze(1)\n",
    "\n",
    "# 5. DataLoader\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds = TensorDataset(X_val_t, y_val_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=128)\n",
    "\n",
    "# 6. CNN Model\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, input_length, num_channels=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, num_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(num_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(num_channels, num_channels*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(num_channels*2, num_channels*4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_channels*4),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_channels*4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNNRegressor(input_length=len(features)).to(device)\n",
    "\n",
    "# 7. Loss and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "# 8. Training with early stopping\n",
    "num_epochs = 200\n",
    "best_mae = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_mae = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_mae += loss.item() * xb.size(0)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_mae = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            val_mae += criterion(preds, yb).item() * xb.size(0)\n",
    "    \n",
    "    train_mae /= len(train_loader.dataset)\n",
    "    val_mae /= len(val_loader.dataset)\n",
    "    scheduler.step(val_mae)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch:3d}/{num_epochs} - Train MAE: {train_mae:.4f} - Val MAE: {val_mae:.4f}')\n",
    "\n",
    "# 9. Test prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_t = X_test_t.to(device)\n",
    "    y_pred = model(X_test_t).cpu().numpy().flatten()\n",
    "\n",
    "# 10. Save submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'yield': y_pred\n",
    "})\n",
    "submission.to_csv('CNN_prediction.csv', index=False)\n",
    "print(f\"Done → CNN predictions saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c37cd9-6540-42c1-8c52-b666ac1c15fd",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79af45d-5270-4db2-a4b3-ba2f822c86b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/500 — Train MAE: 5986.10  |  Val MAE: 6004.67\n",
      "Epoch 10/500 — Train MAE: 5927.44  |  Val MAE: 5940.97\n",
      "Epoch 15/500 — Train MAE: 5832.06  |  Val MAE: 5847.16\n",
      "Epoch 20/500 — Train MAE: 5702.14  |  Val MAE: 5711.24\n",
      "Epoch 25/500 — Train MAE: 5540.48  |  Val MAE: 5546.87\n",
      "Epoch 30/500 — Train MAE: 5342.49  |  Val MAE: 5347.53\n",
      "Epoch 35/500 — Train MAE: 5129.99  |  Val MAE: 5140.02\n",
      "Epoch 40/500 — Train MAE: 4872.22  |  Val MAE: 4870.95\n",
      "Epoch 45/500 — Train MAE: 4599.44  |  Val MAE: 4616.84\n",
      "Epoch 50/500 — Train MAE: 4311.65  |  Val MAE: 4316.42\n",
      "Epoch 55/500 — Train MAE: 3973.09  |  Val MAE: 3955.39\n",
      "Epoch 60/500 — Train MAE: 3639.88  |  Val MAE: 3659.67\n",
      "Epoch 65/500 — Train MAE: 3241.05  |  Val MAE: 3289.30\n",
      "Epoch 70/500 — Train MAE: 2875.15  |  Val MAE: 2819.38\n",
      "Epoch 75/500 — Train MAE: 2456.98  |  Val MAE: 2404.19\n",
      "Epoch 80/500 — Train MAE: 2041.62  |  Val MAE: 1957.86\n",
      "Epoch 85/500 — Train MAE: 1660.85  |  Val MAE: 1643.17\n",
      "Epoch 90/500 — Train MAE: 1273.92  |  Val MAE: 1273.75\n",
      "Epoch 95/500 — Train MAE: 1106.36  |  Val MAE: 873.38\n",
      "Epoch 100/500 — Train MAE: 724.70  |  Val MAE: 569.42\n",
      "Epoch 105/500 — Train MAE: 659.23  |  Val MAE: 410.20\n",
      "Epoch 110/500 — Train MAE: 641.79  |  Val MAE: 324.71\n",
      "Epoch 115/500 — Train MAE: 609.38  |  Val MAE: 296.41\n",
      "Epoch 120/500 — Train MAE: 589.00  |  Val MAE: 272.76\n",
      "Epoch 125/500 — Train MAE: 568.33  |  Val MAE: 272.90\n",
      "Epoch 130/500 — Train MAE: 557.74  |  Val MAE: 269.08\n",
      "Epoch 135/500 — Train MAE: 545.30  |  Val MAE: 272.66\n",
      "Epoch 140/500 — Train MAE: 547.19  |  Val MAE: 264.96\n",
      "Epoch 145/500 — Train MAE: 543.34  |  Val MAE: 277.50\n",
      "Epoch 150/500 — Train MAE: 539.11  |  Val MAE: 262.72\n",
      "Epoch 155/500 — Train MAE: 543.37  |  Val MAE: 266.71\n",
      "Epoch 160/500 — Train MAE: 545.00  |  Val MAE: 263.03\n",
      "Epoch 165/500 — Train MAE: 536.34  |  Val MAE: 267.61\n",
      "Epoch 170/500 — Train MAE: 542.12  |  Val MAE: 264.17\n",
      "Epoch 175/500 — Train MAE: 536.57  |  Val MAE: 262.70\n",
      "Epoch 180/500 — Train MAE: 531.19  |  Val MAE: 265.69\n",
      "Epoch 185/500 — Train MAE: 534.28  |  Val MAE: 286.39\n",
      "Epoch 190/500 — Train MAE: 530.70  |  Val MAE: 262.41\n",
      "Epoch 195/500 — Train MAE: 534.05  |  Val MAE: 263.43\n",
      "Epoch 200/500 — Train MAE: 538.87  |  Val MAE: 267.03\n",
      "Epoch 205/500 — Train MAE: 533.65  |  Val MAE: 275.65\n",
      "Epoch 210/500 — Train MAE: 537.17  |  Val MAE: 265.51\n",
      "Epoch 215/500 — Train MAE: 539.13  |  Val MAE: 267.36\n",
      "Epoch 220/500 — Train MAE: 532.13  |  Val MAE: 280.03\n",
      "Epoch 225/500 — Train MAE: 541.06  |  Val MAE: 262.57\n",
      "Epoch 230/500 — Train MAE: 545.08  |  Val MAE: 261.50\n",
      "Epoch 235/500 — Train MAE: 530.04  |  Val MAE: 273.59\n",
      "Epoch 240/500 — Train MAE: 544.65  |  Val MAE: 264.18\n",
      "Epoch 245/500 — Train MAE: 536.97  |  Val MAE: 280.70\n",
      "Epoch 250/500 — Train MAE: 534.84  |  Val MAE: 274.03\n",
      "Epoch 255/500 — Train MAE: 537.62  |  Val MAE: 261.49\n",
      "Epoch 260/500 — Train MAE: 530.52  |  Val MAE: 261.97\n",
      "Epoch 265/500 — Train MAE: 532.82  |  Val MAE: 264.59\n",
      "Epoch 270/500 — Train MAE: 538.41  |  Val MAE: 260.21\n",
      "Epoch 275/500 — Train MAE: 534.69  |  Val MAE: 264.65\n",
      "Epoch 280/500 — Train MAE: 534.11  |  Val MAE: 261.26\n",
      "Epoch 285/500 — Train MAE: 545.74  |  Val MAE: 284.75\n",
      "Epoch 290/500 — Train MAE: 531.89  |  Val MAE: 263.11\n",
      "Epoch 295/500 — Train MAE: 541.12  |  Val MAE: 270.69\n",
      "Epoch 300/500 — Train MAE: 529.81  |  Val MAE: 263.49\n",
      "Epoch 305/500 — Train MAE: 533.27  |  Val MAE: 294.89\n",
      "Epoch 310/500 — Train MAE: 542.81  |  Val MAE: 261.54\n",
      "Epoch 315/500 — Train MAE: 535.86  |  Val MAE: 263.33\n",
      "Epoch 320/500 — Train MAE: 536.39  |  Val MAE: 280.12\n",
      "Epoch 325/500 — Train MAE: 536.52  |  Val MAE: 304.08\n",
      "Epoch 330/500 — Train MAE: 535.85  |  Val MAE: 262.19\n",
      "Epoch 335/500 — Train MAE: 539.74  |  Val MAE: 266.01\n",
      "Epoch 340/500 — Train MAE: 533.63  |  Val MAE: 266.43\n",
      "Epoch 345/500 — Train MAE: 543.53  |  Val MAE: 266.41\n",
      "Epoch 350/500 — Train MAE: 537.95  |  Val MAE: 264.98\n",
      "Epoch 355/500 — Train MAE: 539.72  |  Val MAE: 280.59\n",
      "Epoch 360/500 — Train MAE: 532.26  |  Val MAE: 263.77\n",
      "Epoch 365/500 — Train MAE: 533.69  |  Val MAE: 287.30\n",
      "Epoch 370/500 — Train MAE: 535.96  |  Val MAE: 265.18\n",
      "Epoch 375/500 — Train MAE: 537.02  |  Val MAE: 272.71\n",
      "Epoch 380/500 — Train MAE: 539.04  |  Val MAE: 262.82\n",
      "Epoch 385/500 — Train MAE: 536.78  |  Val MAE: 273.74\n",
      "Epoch 390/500 — Train MAE: 532.66  |  Val MAE: 267.97\n",
      "Epoch 395/500 — Train MAE: 529.78  |  Val MAE: 287.21\n",
      "Epoch 400/500 — Train MAE: 535.62  |  Val MAE: 260.81\n",
      "Epoch 405/500 — Train MAE: 545.13  |  Val MAE: 263.13\n",
      "Epoch 410/500 — Train MAE: 540.26  |  Val MAE: 267.51\n",
      "Epoch 415/500 — Train MAE: 536.41  |  Val MAE: 269.11\n",
      "Epoch 420/500 — Train MAE: 536.49  |  Val MAE: 267.82\n",
      "Epoch 425/500 — Train MAE: 535.16  |  Val MAE: 284.72\n",
      "Epoch 430/500 — Train MAE: 533.72  |  Val MAE: 278.54\n",
      "Epoch 435/500 — Train MAE: 541.80  |  Val MAE: 312.77\n",
      "Epoch 440/500 — Train MAE: 538.44  |  Val MAE: 281.62\n",
      "Epoch 445/500 — Train MAE: 539.01  |  Val MAE: 260.85\n",
      "Epoch 450/500 — Train MAE: 533.48  |  Val MAE: 262.04\n",
      "Epoch 455/500 — Train MAE: 532.07  |  Val MAE: 269.84\n",
      "Epoch 460/500 — Train MAE: 542.19  |  Val MAE: 280.17\n",
      "Epoch 465/500 — Train MAE: 536.77  |  Val MAE: 265.32\n",
      "Epoch 470/500 — Train MAE: 529.56  |  Val MAE: 274.65\n",
      "Epoch 475/500 — Train MAE: 539.26  |  Val MAE: 259.03\n",
      "Epoch 480/500 — Train MAE: 533.90  |  Val MAE: 260.29\n",
      "Epoch 485/500 — Train MAE: 545.41  |  Val MAE: 263.30\n",
      "Epoch 490/500 — Train MAE: 537.90  |  Val MAE: 260.87\n",
      "Epoch 495/500 — Train MAE: 541.39  |  Val MAE: 270.51\n",
      "Epoch 500/500 — Train MAE: 536.95  |  Val MAE: 268.81\n",
      "▶ Best validation MAE: 258.89\n",
      "✅ Predictions saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Feature selection\n",
    "features = [c for c in train_df.columns if c not in ('id','Row#','yield')]\n",
    "\n",
    "# Prepare training data\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df['yield'].values\n",
    "\n",
    "# Standardize using only training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Prepare training dataset and loaders\n",
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train_scaled).float().unsqueeze(-1),\n",
    "    torch.from_numpy(y_train).float().unsqueeze(1)\n",
    ")\n",
    "n_val = int(len(train_dataset) * 0.2)\n",
    "n_tr = len(train_dataset) - n_val\n",
    "train_ds, val_ds = random_split(train_dataset, [n_tr, n_val])\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "# Define model\n",
    "class RNNRegressor(nn.Module):\n",
    "    def __init__(self, seq_len, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.lstm = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size * self.num_directions, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        last = h_n.view(self.lstm.num_layers, self.num_directions, x.size(0), self.lstm.hidden_size)[-1]\n",
    "        last = last.transpose(0, 1).contiguous().view(x.size(0), -1)\n",
    "        x = torch.relu(self.bn1(self.fc1(last)))\n",
    "        x = self.drop(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNNRegressor(seq_len=len(features)).to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "best_val = float('inf')\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_mae = run_epoch(train_loader, train=True)\n",
    "    val_mae = run_epoch(val_loader, train=False)\n",
    "    scheduler.step(val_mae)\n",
    "    if val_mae < best_val:\n",
    "        best_val = val_mae\n",
    "        torch.save(model.state_dict(), 'best_rnn.pth')\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:2d}/{num_epochs} — Train MAE: {train_mae:.2f}  |  Val MAE: {val_mae:.2f}\")\n",
    "print(f\"▶ Best validation MAE: {best_val:.2f}\")\n",
    "\n",
    "# --- Inference on test set ---\n",
    "model.load_state_dict(torch.load('best_rnn.pth'))\n",
    "\n",
    "X_test = test_df[features].values\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "test_tensor = torch.from_numpy(X_test_scaled).float().unsqueeze(-1).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = model(test_tensor).cpu().numpy().squeeze()\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'yield': test_preds\n",
    "})\n",
    "submission.to_csv('RNN_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb164f4e-fe98-4a79-b986-1ad065496c1e",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eb87c4e-2879-4bd6-9a04-927bb918e8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/500 — Train MAE: 5994.45 | Val MAE: 5963.30\n",
      "Epoch 10/500 — Train MAE: 5932.53 | Val MAE: 5896.19\n",
      "Epoch 15/500 — Train MAE: 5836.83 | Val MAE: 5801.42\n",
      "Epoch 20/500 — Train MAE: 5707.04 | Val MAE: 5668.41\n",
      "Epoch 25/500 — Train MAE: 5545.16 | Val MAE: 5503.42\n",
      "Epoch 30/500 — Train MAE: 5355.18 | Val MAE: 5301.85\n",
      "Epoch 35/500 — Train MAE: 5133.67 | Val MAE: 5081.63\n",
      "Epoch 40/500 — Train MAE: 4882.31 | Val MAE: 4862.46\n",
      "Epoch 45/500 — Train MAE: 4684.94 | Val MAE: 4644.23\n",
      "Epoch 50/500 — Train MAE: 4418.45 | Val MAE: 4392.07\n",
      "Epoch 55/500 — Train MAE: 4069.29 | Val MAE: 4015.41\n",
      "Epoch 60/500 — Train MAE: 3757.99 | Val MAE: 3703.87\n",
      "Epoch 65/500 — Train MAE: 3349.78 | Val MAE: 3319.20\n",
      "Epoch 70/500 — Train MAE: 3007.07 | Val MAE: 2956.64\n",
      "Epoch 75/500 — Train MAE: 2593.40 | Val MAE: 2514.20\n",
      "Epoch 80/500 — Train MAE: 2203.90 | Val MAE: 2123.79\n",
      "Epoch 85/500 — Train MAE: 1734.51 | Val MAE: 1659.52\n",
      "Epoch 90/500 — Train MAE: 1368.05 | Val MAE: 1319.12\n",
      "Epoch 95/500 — Train MAE: 1083.41 | Val MAE: 1022.51\n",
      "Epoch 100/500 — Train MAE: 808.48 | Val MAE: 646.21\n",
      "Epoch 105/500 — Train MAE: 678.42 | Val MAE: 420.75\n",
      "Epoch 110/500 — Train MAE: 634.49 | Val MAE: 350.14\n",
      "Epoch 115/500 — Train MAE: 585.60 | Val MAE: 281.77\n",
      "Epoch 120/500 — Train MAE: 582.37 | Val MAE: 264.28\n",
      "Epoch 125/500 — Train MAE: 572.87 | Val MAE: 290.16\n",
      "Epoch 130/500 — Train MAE: 566.35 | Val MAE: 269.22\n",
      "Epoch 135/500 — Train MAE: 570.47 | Val MAE: 270.76\n",
      "Epoch 140/500 — Train MAE: 555.07 | Val MAE: 268.80\n",
      "Epoch 145/500 — Train MAE: 553.39 | Val MAE: 272.16\n",
      "Epoch 150/500 — Train MAE: 557.36 | Val MAE: 261.60\n",
      "Epoch 155/500 — Train MAE: 561.75 | Val MAE: 261.84\n",
      "Epoch 160/500 — Train MAE: 557.99 | Val MAE: 258.59\n",
      "Epoch 165/500 — Train MAE: 551.44 | Val MAE: 280.17\n",
      "Epoch 170/500 — Train MAE: 556.32 | Val MAE: 267.96\n",
      "Epoch 175/500 — Train MAE: 555.97 | Val MAE: 261.95\n",
      "Epoch 180/500 — Train MAE: 560.10 | Val MAE: 252.39\n",
      "Epoch 185/500 — Train MAE: 558.11 | Val MAE: 261.54\n",
      "Epoch 190/500 — Train MAE: 554.48 | Val MAE: 257.14\n",
      "Epoch 195/500 — Train MAE: 562.65 | Val MAE: 257.61\n",
      "Epoch 200/500 — Train MAE: 564.15 | Val MAE: 272.37\n",
      "Epoch 205/500 — Train MAE: 560.15 | Val MAE: 254.93\n",
      "Epoch 210/500 — Train MAE: 556.80 | Val MAE: 252.91\n",
      "Epoch 215/500 — Train MAE: 553.56 | Val MAE: 254.30\n",
      "Epoch 220/500 — Train MAE: 563.22 | Val MAE: 256.18\n",
      "Epoch 225/500 — Train MAE: 558.78 | Val MAE: 258.12\n",
      "Epoch 230/500 — Train MAE: 558.46 | Val MAE: 260.74\n",
      "Epoch 235/500 — Train MAE: 558.88 | Val MAE: 262.34\n",
      "Epoch 240/500 — Train MAE: 554.63 | Val MAE: 263.17\n",
      "Epoch 245/500 — Train MAE: 564.38 | Val MAE: 261.96\n",
      "Epoch 250/500 — Train MAE: 560.77 | Val MAE: 256.72\n",
      "Epoch 255/500 — Train MAE: 561.74 | Val MAE: 260.59\n",
      "Epoch 260/500 — Train MAE: 559.78 | Val MAE: 263.91\n",
      "Epoch 265/500 — Train MAE: 562.00 | Val MAE: 256.21\n",
      "Epoch 270/500 — Train MAE: 557.47 | Val MAE: 264.87\n",
      "Epoch 275/500 — Train MAE: 560.09 | Val MAE: 258.81\n",
      "Epoch 280/500 — Train MAE: 552.43 | Val MAE: 269.37\n",
      "Epoch 285/500 — Train MAE: 553.02 | Val MAE: 256.81\n",
      "Epoch 290/500 — Train MAE: 557.04 | Val MAE: 254.64\n",
      "Epoch 295/500 — Train MAE: 558.98 | Val MAE: 253.50\n",
      "Epoch 300/500 — Train MAE: 558.00 | Val MAE: 262.92\n",
      "Epoch 305/500 — Train MAE: 555.78 | Val MAE: 262.03\n",
      "Epoch 310/500 — Train MAE: 558.14 | Val MAE: 255.28\n",
      "Epoch 315/500 — Train MAE: 563.78 | Val MAE: 266.05\n",
      "Epoch 320/500 — Train MAE: 561.15 | Val MAE: 262.90\n",
      "Epoch 325/500 — Train MAE: 553.45 | Val MAE: 259.00\n",
      "Epoch 330/500 — Train MAE: 553.35 | Val MAE: 260.30\n",
      "Epoch 335/500 — Train MAE: 554.99 | Val MAE: 257.90\n",
      "Epoch 340/500 — Train MAE: 563.08 | Val MAE: 260.37\n",
      "Epoch 345/500 — Train MAE: 555.09 | Val MAE: 261.58\n",
      "Epoch 350/500 — Train MAE: 560.69 | Val MAE: 256.91\n",
      "Epoch 355/500 — Train MAE: 561.38 | Val MAE: 252.13\n",
      "Epoch 360/500 — Train MAE: 552.37 | Val MAE: 257.37\n",
      "Epoch 365/500 — Train MAE: 556.99 | Val MAE: 256.52\n",
      "Epoch 370/500 — Train MAE: 556.72 | Val MAE: 266.15\n",
      "Epoch 375/500 — Train MAE: 557.17 | Val MAE: 266.47\n",
      "Epoch 380/500 — Train MAE: 557.46 | Val MAE: 259.26\n",
      "Epoch 385/500 — Train MAE: 553.04 | Val MAE: 256.32\n",
      "Epoch 390/500 — Train MAE: 555.47 | Val MAE: 267.65\n",
      "Epoch 395/500 — Train MAE: 552.33 | Val MAE: 258.51\n",
      "Epoch 400/500 — Train MAE: 551.39 | Val MAE: 266.86\n",
      "Epoch 405/500 — Train MAE: 559.64 | Val MAE: 266.15\n",
      "Epoch 410/500 — Train MAE: 548.68 | Val MAE: 257.44\n",
      "Epoch 415/500 — Train MAE: 564.39 | Val MAE: 256.44\n",
      "Epoch 420/500 — Train MAE: 558.84 | Val MAE: 259.57\n",
      "Epoch 425/500 — Train MAE: 559.57 | Val MAE: 254.10\n",
      "Epoch 430/500 — Train MAE: 553.61 | Val MAE: 258.09\n",
      "Epoch 435/500 — Train MAE: 557.22 | Val MAE: 272.95\n",
      "Epoch 440/500 — Train MAE: 555.76 | Val MAE: 256.90\n",
      "Epoch 445/500 — Train MAE: 559.18 | Val MAE: 258.04\n",
      "Epoch 450/500 — Train MAE: 553.87 | Val MAE: 261.86\n",
      "Epoch 455/500 — Train MAE: 555.48 | Val MAE: 255.86\n",
      "Epoch 460/500 — Train MAE: 555.95 | Val MAE: 278.42\n",
      "Epoch 465/500 — Train MAE: 563.81 | Val MAE: 255.51\n",
      "Epoch 470/500 — Train MAE: 560.96 | Val MAE: 279.10\n",
      "Epoch 475/500 — Train MAE: 561.40 | Val MAE: 257.73\n",
      "Epoch 480/500 — Train MAE: 565.53 | Val MAE: 252.22\n",
      "Epoch 485/500 — Train MAE: 562.20 | Val MAE: 269.89\n",
      "Epoch 490/500 — Train MAE: 555.97 | Val MAE: 253.70\n",
      "Epoch 495/500 — Train MAE: 557.68 | Val MAE: 263.00\n",
      "Epoch 500/500 — Train MAE: 557.85 | Val MAE: 253.97\n",
      "Best Validation MAE: 251.76\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Select features\n",
    "features = [c for c in train_df.columns if c not in ('id', 'Row#', 'yield')]\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df['yield'].values\n",
    "\n",
    "# Fit scaler only on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Prepare PyTorch datasets\n",
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train_scaled).float().unsqueeze(-1),\n",
    "    torch.from_numpy(y_train).float().unsqueeze(1)\n",
    ")\n",
    "\n",
    "# Train-validation split\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "# Define LSTM Regressor\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, seq_len, hidden_size=128, num_layers=2, dropout=0.2, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional, batch_first=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size * self.num_directions, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h_last = h_n.view(self.lstm.num_layers, self.num_directions, x.size(0), self.lstm.hidden_size)[-1]\n",
    "        h_last = h_last.transpose(0, 1).reshape(x.size(0), -1)\n",
    "        x = torch.relu(self.bn1(self.fc1(h_last)))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMRegressor(seq_len=len(features)).to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "best_val_mae = float('inf')\n",
    "for epoch in range(1, 501):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_mae = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            val_loss += criterion(preds, yb).item() * xb.size(0)\n",
    "    val_mae = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    scheduler.step(val_mae)\n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        torch.save(model.state_dict(), 'best_lstm.pth')\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch:2d}/500 — Train MAE: {train_mae:.2f} | Val MAE: {val_mae:.2f}')\n",
    "\n",
    "print(f'Best Validation MAE: {best_val_mae:.2f}')\n",
    "\n",
    "# --- Inference on test.csv ---\n",
    "model.load_state_dict(torch.load('best_lstm.pth'))\n",
    "model.eval()\n",
    "\n",
    "X_test = test_df[features].values\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_tensor = torch.from_numpy(X_test_scaled).float().unsqueeze(-1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = model(X_test_tensor).cpu().numpy().squeeze()\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'yield': test_preds\n",
    "})\n",
    "submission.to_csv('LSTM_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec22fb-d845-4835-8411-de2c16341a33",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5203a9c2-1041-4a17-b7c5-64205fd8705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/500 — Train MAE: 5749.81 | Val MAE: 5700.79\n",
      "Epoch  10/500 — Train MAE: 4560.94 | Val MAE: 4375.72\n",
      "Epoch  15/500 — Train MAE: 1898.07 | Val MAE: 1578.66\n",
      "Epoch  20/500 — Train MAE: 543.71 | Val MAE: 345.77\n",
      "Epoch  25/500 — Train MAE: 500.65 | Val MAE: 282.29\n",
      "Epoch  30/500 — Train MAE: 487.36 | Val MAE: 280.92\n",
      "Epoch  35/500 — Train MAE: 489.57 | Val MAE: 278.56\n",
      "Epoch  40/500 — Train MAE: 477.47 | Val MAE: 288.34\n",
      "Epoch  45/500 — Train MAE: 476.31 | Val MAE: 270.45\n",
      "Epoch  50/500 — Train MAE: 468.04 | Val MAE: 270.17\n",
      "Epoch  55/500 — Train MAE: 474.57 | Val MAE: 268.49\n",
      "Epoch  60/500 — Train MAE: 473.38 | Val MAE: 264.54\n",
      "Epoch  65/500 — Train MAE: 462.77 | Val MAE: 269.48\n",
      "Epoch  70/500 — Train MAE: 466.37 | Val MAE: 269.74\n",
      "Epoch  75/500 — Train MAE: 461.49 | Val MAE: 268.75\n",
      "Epoch  80/500 — Train MAE: 467.85 | Val MAE: 266.98\n",
      "Epoch  85/500 — Train MAE: 466.92 | Val MAE: 266.44\n",
      "Epoch  90/500 — Train MAE: 464.27 | Val MAE: 266.40\n",
      "Epoch  95/500 — Train MAE: 470.77 | Val MAE: 265.69\n",
      "Epoch 100/500 — Train MAE: 466.52 | Val MAE: 265.50\n",
      "Epoch 105/500 — Train MAE: 466.30 | Val MAE: 265.67\n",
      "Epoch 110/500 — Train MAE: 469.07 | Val MAE: 265.57\n",
      "Epoch 115/500 — Train MAE: 464.68 | Val MAE: 265.59\n",
      "Epoch 120/500 — Train MAE: 466.57 | Val MAE: 265.63\n",
      "Epoch 125/500 — Train MAE: 470.21 | Val MAE: 265.65\n",
      "Epoch 130/500 — Train MAE: 469.66 | Val MAE: 265.63\n",
      "Epoch 135/500 — Train MAE: 468.69 | Val MAE: 265.63\n",
      "Epoch 140/500 — Train MAE: 468.26 | Val MAE: 265.63\n",
      "Epoch 145/500 — Train MAE: 468.40 | Val MAE: 265.63\n",
      "Epoch 150/500 — Train MAE: 472.31 | Val MAE: 265.63\n",
      "Epoch 155/500 — Train MAE: 463.05 | Val MAE: 265.62\n",
      "Epoch 160/500 — Train MAE: 470.34 | Val MAE: 265.62\n",
      "Epoch 165/500 — Train MAE: 463.91 | Val MAE: 265.63\n",
      "Epoch 170/500 — Train MAE: 470.17 | Val MAE: 265.63\n",
      "Epoch 175/500 — Train MAE: 468.72 | Val MAE: 265.64\n",
      "Epoch 180/500 — Train MAE: 471.37 | Val MAE: 265.64\n",
      "Epoch 185/500 — Train MAE: 466.34 | Val MAE: 265.64\n",
      "Epoch 190/500 — Train MAE: 468.74 | Val MAE: 265.65\n",
      "Epoch 195/500 — Train MAE: 464.32 | Val MAE: 265.64\n",
      "Epoch 200/500 — Train MAE: 467.42 | Val MAE: 265.64\n",
      "Epoch 205/500 — Train MAE: 470.06 | Val MAE: 265.65\n",
      "Epoch 210/500 — Train MAE: 464.76 | Val MAE: 265.63\n",
      "Epoch 215/500 — Train MAE: 465.17 | Val MAE: 265.65\n",
      "Epoch 220/500 — Train MAE: 465.82 | Val MAE: 265.65\n",
      "Epoch 225/500 — Train MAE: 467.22 | Val MAE: 265.66\n",
      "Epoch 230/500 — Train MAE: 466.39 | Val MAE: 265.67\n",
      "Epoch 235/500 — Train MAE: 470.09 | Val MAE: 265.68\n",
      "Epoch 240/500 — Train MAE: 462.18 | Val MAE: 265.69\n",
      "Epoch 245/500 — Train MAE: 472.58 | Val MAE: 265.68\n",
      "Epoch 250/500 — Train MAE: 459.97 | Val MAE: 265.68\n",
      "Epoch 255/500 — Train MAE: 466.25 | Val MAE: 265.69\n",
      "Epoch 260/500 — Train MAE: 463.91 | Val MAE: 265.70\n",
      "Epoch 265/500 — Train MAE: 463.64 | Val MAE: 265.70\n",
      "Epoch 270/500 — Train MAE: 462.93 | Val MAE: 265.69\n",
      "Epoch 275/500 — Train MAE: 463.91 | Val MAE: 265.70\n",
      "Epoch 280/500 — Train MAE: 469.43 | Val MAE: 265.70\n",
      "Epoch 285/500 — Train MAE: 462.63 | Val MAE: 265.70\n",
      "Epoch 290/500 — Train MAE: 464.16 | Val MAE: 265.69\n",
      "Epoch 295/500 — Train MAE: 469.76 | Val MAE: 265.70\n",
      "Epoch 300/500 — Train MAE: 468.45 | Val MAE: 265.70\n",
      "Epoch 305/500 — Train MAE: 470.81 | Val MAE: 265.70\n",
      "Epoch 310/500 — Train MAE: 470.67 | Val MAE: 265.71\n",
      "Epoch 315/500 — Train MAE: 463.49 | Val MAE: 265.72\n",
      "Epoch 320/500 — Train MAE: 463.71 | Val MAE: 265.73\n",
      "Epoch 325/500 — Train MAE: 469.13 | Val MAE: 265.72\n",
      "Epoch 330/500 — Train MAE: 471.45 | Val MAE: 265.72\n",
      "Epoch 335/500 — Train MAE: 470.61 | Val MAE: 265.73\n",
      "Epoch 340/500 — Train MAE: 461.87 | Val MAE: 265.73\n",
      "Epoch 345/500 — Train MAE: 466.56 | Val MAE: 265.73\n",
      "Epoch 350/500 — Train MAE: 463.72 | Val MAE: 265.73\n",
      "Epoch 355/500 — Train MAE: 464.60 | Val MAE: 265.72\n",
      "Epoch 360/500 — Train MAE: 466.75 | Val MAE: 265.72\n",
      "Epoch 365/500 — Train MAE: 464.70 | Val MAE: 265.74\n",
      "Epoch 370/500 — Train MAE: 460.26 | Val MAE: 265.73\n",
      "Epoch 375/500 — Train MAE: 462.71 | Val MAE: 265.74\n",
      "Epoch 380/500 — Train MAE: 468.65 | Val MAE: 265.74\n",
      "Epoch 385/500 — Train MAE: 469.12 | Val MAE: 265.74\n",
      "Epoch 390/500 — Train MAE: 466.59 | Val MAE: 265.75\n",
      "Epoch 395/500 — Train MAE: 469.56 | Val MAE: 265.74\n",
      "Epoch 400/500 — Train MAE: 461.33 | Val MAE: 265.74\n",
      "Epoch 405/500 — Train MAE: 464.52 | Val MAE: 265.73\n",
      "Epoch 410/500 — Train MAE: 468.70 | Val MAE: 265.75\n",
      "Epoch 415/500 — Train MAE: 459.24 | Val MAE: 265.75\n",
      "Epoch 420/500 — Train MAE: 468.29 | Val MAE: 265.75\n",
      "Epoch 425/500 — Train MAE: 467.45 | Val MAE: 265.75\n",
      "Epoch 430/500 — Train MAE: 464.27 | Val MAE: 265.76\n",
      "Epoch 435/500 — Train MAE: 468.74 | Val MAE: 265.76\n",
      "Epoch 440/500 — Train MAE: 468.80 | Val MAE: 265.76\n",
      "Epoch 445/500 — Train MAE: 463.87 | Val MAE: 265.76\n",
      "Epoch 450/500 — Train MAE: 462.68 | Val MAE: 265.76\n",
      "Epoch 455/500 — Train MAE: 468.46 | Val MAE: 265.76\n",
      "Epoch 460/500 — Train MAE: 474.61 | Val MAE: 265.76\n",
      "Epoch 465/500 — Train MAE: 465.02 | Val MAE: 265.77\n",
      "Epoch 470/500 — Train MAE: 465.12 | Val MAE: 265.77\n",
      "Epoch 475/500 — Train MAE: 460.69 | Val MAE: 265.77\n",
      "Epoch 480/500 — Train MAE: 462.27 | Val MAE: 265.78\n",
      "Epoch 485/500 — Train MAE: 463.55 | Val MAE: 265.77\n",
      "Epoch 490/500 — Train MAE: 472.20 | Val MAE: 265.78\n",
      "Epoch 495/500 — Train MAE: 463.03 | Val MAE: 265.78\n",
      "Epoch 500/500 — Train MAE: 463.02 | Val MAE: 265.78\n",
      "▶ Best Validation MAE: 264.53\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "features = [c for c in df.columns if c not in ('id', 'Row#', 'yield')]\n",
    "X = df[features].values\n",
    "y = df['yield'].values\n",
    "X_test = test_df[features].values\n",
    "test_ids = test_df['id'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2. Prepare PyTorch datasets\n",
    "data = TensorDataset(\n",
    "    torch.from_numpy(X).float().unsqueeze(-1),\n",
    "    torch.from_numpy(y).float().unsqueeze(1)\n",
    ")\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size   = len(data) - train_size\n",
    "train_ds, val_ds = random_split(data, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64)\n",
    "test_loader  = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_test).float().unsqueeze(-1)),\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# 3. Positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "\n",
    "# 4. Transformer regressor\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, seq_len, d_model=64, nhead=8,\n",
    "                 num_layers=2, dim_ff=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(1, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=seq_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer,\n",
    "                                             num_layers=num_layers)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)             # (batch, seq_len, d_model)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.encoder(x)           # (batch, seq_len, d_model)\n",
    "        x = x.mean(dim=1)             # Global average pooling\n",
    "        return self.regressor(x)\n",
    "\n",
    "# 5. Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerRegressor(seq_len=len(features)).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# 6. Training & validation loops\n",
    "best_val_mae = float('inf')\n",
    "for epoch in range(1, 501):  # 100 epochs\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_mae = train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            val_loss += criterion(preds, yb).item() * xb.size(0)\n",
    "    val_mae = val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    scheduler.step(val_mae)\n",
    "    \n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        torch.save(model.state_dict(), 'best_transformer.pth')\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch:3d}/500 — Train MAE: {train_mae:.2f} | Val MAE: {val_mae:.2f}')\n",
    "\n",
    "print(f\"▶ Best Validation MAE: {best_val_mae:.2f}\")\n",
    "\n",
    "# 7. Inference on test set\n",
    "model.load_state_dict(torch.load('best_transformer.pth'))\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for xb in test_loader:\n",
    "        xb = xb[0].to(device)\n",
    "        preds.append(model(xb).cpu().numpy())\n",
    "preds = np.vstack(preds).flatten()\n",
    "\n",
    "# 8. Save submission\n",
    "submission = pd.DataFrame({'id': test_ids, 'yield': preds})\n",
    "submission.to_csv('Transformer_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb0998-b88d-4d28-91d2-9e9bef7b41bd",
   "metadata": {},
   "source": [
    "# Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "834a7874-4d44-450e-83ed-990ea03d4461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/201 - Train MAE: 5446.97 | Val MAE: 5342.18\n",
      "Epoch 10/201 - Train MAE: 2927.53 | Val MAE: 2582.31\n",
      "Epoch 15/201 - Train MAE: 780.40 | Val MAE: 477.29\n",
      "Epoch 20/201 - Train MAE: 641.29 | Val MAE: 336.35\n",
      "Epoch 25/201 - Train MAE: 632.57 | Val MAE: 301.21\n",
      "Epoch 30/201 - Train MAE: 629.88 | Val MAE: 283.80\n",
      "Epoch 35/201 - Train MAE: 617.93 | Val MAE: 265.28\n",
      "Epoch 40/201 - Train MAE: 617.97 | Val MAE: 271.09\n",
      "Epoch 45/201 - Train MAE: 620.80 | Val MAE: 267.66\n",
      "Epoch 50/201 - Train MAE: 619.95 | Val MAE: 276.18\n",
      "Epoch 55/201 - Train MAE: 610.00 | Val MAE: 266.24\n",
      "Epoch 60/201 - Train MAE: 616.07 | Val MAE: 268.45\n",
      "Epoch 65/201 - Train MAE: 604.84 | Val MAE: 268.92\n",
      "Epoch 70/201 - Train MAE: 618.20 | Val MAE: 270.47\n",
      "Epoch 75/201 - Train MAE: 611.60 | Val MAE: 269.29\n",
      "Epoch 80/201 - Train MAE: 615.94 | Val MAE: 269.13\n",
      "Epoch 85/201 - Train MAE: 609.41 | Val MAE: 269.14\n",
      "Epoch 90/201 - Train MAE: 613.43 | Val MAE: 269.28\n",
      "Epoch 95/201 - Train MAE: 614.83 | Val MAE: 270.35\n",
      "Best Validation MAE: 265.28\n",
      "Done → saved hybrid model predictions to submission.csv\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df  = pd.read_csv('data/test.csv')\n",
    "features = [c for c in train_df.columns if c not in ('id','Row#','yield')]\n",
    "\n",
    "X = train_df[features].values\n",
    "y = train_df['yield'].values\n",
    "X_test = test_df[features].values\n",
    "test_ids = test_df['id'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X      = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2. Create PyTorch datasets and loaders\n",
    "data = TensorDataset(torch.from_numpy(X).float().unsqueeze(-1),\n",
    "                     torch.from_numpy(y).float().unsqueeze(1))\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size   = len(data) - train_size\n",
    "train_ds, val_ds = random_split(data, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64)\n",
    "test_loader  = DataLoader(TensorDataset(torch.from_numpy(X_test).float().unsqueeze(-1)),\n",
    "                          batch_size=64)\n",
    "\n",
    "# 3. Define a CNN + LSTM hybrid regressor\n",
    "class HybridRegressor(nn.Module):\n",
    "    def __init__(self, seq_len, cnn_channels=(16,32), \n",
    "                 lstm_hidden=64, lstm_layers=2, bidir=True):\n",
    "        super().__init__()\n",
    "        # CNN branch\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(1, cnn_channels[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(cnn_channels[0], cnn_channels[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        # LSTM branch (on CNN features)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_channels[1],\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidir,\n",
    "            dropout=0.2 if lstm_layers>1 else 0.0\n",
    "        )\n",
    "        self.bidir = bidir\n",
    "        # Final MLP\n",
    "        mlp_in = lstm_hidden * (2 if bidir else 1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(mlp_in, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, 1) -> CNN expects (batch, 1, seq_len)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.cnn(x)\n",
    "        # x: (batch, channels, seq_len/4) -> back to (batch, seq_len/4, channels)\n",
    "        x = x.transpose(1,2)\n",
    "        # LSTM\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # pick last layer hidden: shape (num_layers * directions, batch, hidden)\n",
    "        if self.bidir:\n",
    "            # get last forward & backward hidden\n",
    "            h_last = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h_last = h_n[-1]\n",
    "        # final regressor\n",
    "        return self.mlp(h_last)\n",
    "\n",
    "# 4. Setup training components\n",
    "device   = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model    = HybridRegressor(seq_len=len(features)).to(device)\n",
    "criterion= nn.L1Loss()\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=5.1e-4, weight_decay=0.5e-5)\n",
    "scheduler= torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                     factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# 5. Training & validation loop\n",
    "best_val = float('inf')\n",
    "for epoch in range(1, 100):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_train = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_train += loss.item() * xb.size(0)\n",
    "    train_mae = total_train / len(train_loader.dataset)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    total_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            total_val += criterion(preds, yb).item() * xb.size(0)\n",
    "    val_mae = total_val / len(val_loader.dataset)\n",
    "\n",
    "    scheduler.step(val_mae)\n",
    "    if val_mae < best_val:\n",
    "        best_val = val_mae\n",
    "        torch.save(model.state_dict(), 'best_hybrid.pth')\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:2d}/201 - Train MAE: {train_mae:.2f} | Val MAE: {val_mae:.2f}\")\n",
    "\n",
    "print(f\"Best Validation MAE: {best_val:.2f}\")\n",
    "\n",
    "# 6. Inference on test set\n",
    "model.load_state_dict(torch.load('best_hybrid.pth'))\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for xb in test_loader:\n",
    "        xb = xb[0].to(device)\n",
    "        preds.append(model(xb).cpu().numpy())\n",
    "preds = np.vstack(preds).flatten()\n",
    "\n",
    "# 7. Save submission\n",
    "pd.DataFrame({'id': test_ids, 'yield': preds}).to_csv('Hybrid_prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
